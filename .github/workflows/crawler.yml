name: Weekly Crawler

on:
  schedule:
    # 매주 월요일 새벽 2시 (KST) = 일요일 17시 (UTC)
    - cron: '0 17 * * 0'
  workflow_dispatch: # 수동 실행 가능

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 180 # 3시간 타임아웃 (전체 페이지 크롤링 고려)

    env:
      NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      CRAWL_DELAY: 0.3
      MAX_RETRIES: 3
      TIMEOUT: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
          cache-dependency-path: crawler/requirements.txt

      - name: Install Chrome and ChromeDriver
        run: |
          # Chrome 설치
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

          # ChromeDriver 버전 확인 및 설치
          CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d'.' -f1)
          echo "Chrome version: $CHROME_VERSION"

          # ChromeDriver 다운로드 (최신 버전)
          wget -N https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/$(curl -s https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_STABLE)/linux64/chromedriver-linux64.zip
          unzip chromedriver-linux64.zip
          sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver
          rm -rf chromedriver-linux64.zip chromedriver-linux64

          # 확인
          google-chrome --version
          chromedriver --version

      - name: Install Python dependencies
        run: |
          cd crawler
          pip install -r requirements.txt

      - name: Run CU crawler
        id: cu-crawler
        continue-on-error: true
        timeout-minutes: 30
        run: |
          cd crawler
          python upload_to_db.py cu

      - name: Run SevenEleven crawler
        id: seven-crawler
        continue-on-error: true
        timeout-minutes: 45
        run: |
          cd crawler
          python upload_to_db.py seven

      - name: Run GS25 crawler
        id: gs25-crawler
        continue-on-error: true
        timeout-minutes: 90
        run: |
          cd crawler
          python upload_to_db.py gs25

      - name: Run Emart24 crawler
        id: emart24-crawler
        continue-on-error: true
        timeout-minutes: 60
        run: |
          cd crawler
          python upload_to_db.py emart24

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: crawler-logs
          path: crawler/logs/
          retention-days: 7
          if-no-files-found: warn

      - name: Upload JSON backups
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: json-backups
          path: crawler/data/
          retention-days: 30
          if-no-files-found: warn

      - name: Parse crawler results
        id: parse-results
        if: always()
        run: |
          echo "=== Parsing Crawler Results ==="

          # 로그 파일에서 결과 파싱 (CRAWLER_RESULTS=... 라인 찾기)
          RESULTS_JSON=""
          if [ -f crawler/logs/upload_to_db.log ]; then
            RESULTS_JSON=$(grep "CRAWLER_RESULTS=" crawler/logs/upload_to_db.log | tail -1 | sed 's/CRAWLER_RESULTS=//')
          fi

          # 결과가 없으면 빈 객체
          if [ -z "$RESULTS_JSON" ]; then
            RESULTS_JSON='{}'
          fi

          echo "results=$RESULTS_JSON" >> $GITHUB_OUTPUT
          echo "Results: $RESULTS_JSON"

          # 신규 프로모션 개수 계산
          TOTAL_NEW=$(echo "$RESULTS_JSON" | jq '[.CU.new, .SevenEleven.new, .GS25.new, .Emart24.new] | map(select(. != null)) | add // 0')
          echo "total_new=$TOTAL_NEW" >> $GITHUB_OUTPUT
          echo "Total new promotions: $TOTAL_NEW"

      - name: Send webhook notification
        if: always() && steps.parse-results.outputs.total_new > 0
        env:
          WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}
          WEBHOOK_SECRET: ${{ secrets.WEBHOOK_SECRET }}
          RESULTS: ${{ steps.parse-results.outputs.results }}
        run: |
          echo "=== Sending Webhook ==="
          echo "Total new: ${{ steps.parse-results.outputs.total_new }}"

          if [ -z "$WEBHOOK_URL" ] || [ -z "$WEBHOOK_SECRET" ]; then
            echo "⚠️ WEBHOOK_URL or WEBHOOK_SECRET not set, skipping notification"
            exit 0
          fi

          curl -X POST "$WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $WEBHOOK_SECRET" \
            -d "{\"results\": $RESULTS}" \
            --fail \
            || echo "⚠️ Webhook failed, but continuing..."

          echo "✅ Webhook sent successfully"

      - name: Check results
        if: always()
        run: |
          echo "=== Crawler Results ==="
          echo "CU: ${{ steps.cu-crawler.outcome }}"
          echo "SevenEleven: ${{ steps.seven-crawler.outcome }}"
          echo "GS25: ${{ steps.gs25-crawler.outcome }}"
          echo "Emart24: ${{ steps.emart24-crawler.outcome }}"
          echo "New promotions: ${{ steps.parse-results.outputs.total_new }}"

          FAILED=0
          if [ "${{ steps.cu-crawler.outcome }}" == "failure" ]; then
            echo "⚠️ CU crawler failed"
            FAILED=1
          fi
          if [ "${{ steps.seven-crawler.outcome }}" == "failure" ]; then
            echo "⚠️ SevenEleven crawler failed"
            FAILED=1
          fi
          if [ "${{ steps.gs25-crawler.outcome }}" == "failure" ]; then
            echo "⚠️ GS25 crawler failed"
            FAILED=1
          fi
          if [ "${{ steps.emart24-crawler.outcome }}" == "failure" ]; then
            echo "⚠️ Emart24 crawler failed"
            FAILED=1
          fi

          if [ $FAILED -eq 1 ]; then
            echo "⚠️ Some crawlers failed, but continuing..."
            # exit 1 대신 경고만 표시 (일부 실패해도 성공한 것은 DB에 저장됨)
          else
            echo "✅ All crawlers completed successfully"
          fi
